<h1 id="warp">Warp</h1>
<p>Authors: Michael Snoyman and Kazu Yamamoto</p>
<p>Warp is a high-performance library of HTTP server side in Haskell, a purely functional programming language. Both Yesod, a web application framework, and <code>mighty</code>, an HTTP server, are implemented over Warp. According to our throughput benchmark, <code>mighty</code> provides performance on par with <code>nginx</code>. This article will explain the architecture of Warp and how we improved its performance. Warp can run on many platforms including Linux, BSD variants, MacOS, and Windows. To make our explanation simple, however, we will talk about Linux only for the rest of this article.</p>
<h2 id="network-programming-in-haskell">Network programming in Haskell</h2>
<p>Some people may still misunderstand that functional programming languages are slow or impractical. However, to our best knowledge, Haskell provides the best scheme for network programming. This is because that GHC (Glasgow Haskell Compiler), the flagship compiler of Haskell, provides lightweight and robust user thread (or sometime called green thread). In this section, we will briefly explain history of network programming in server side.</p>
<h3 id="native-threads">Native threads</h3>
<p>Traditional servers use a technique called thread programming. In this architecture, each connection is handled by a single process or native thread (or sometime called OS thread)</p>
<p>This architecture can be broken down by how to create processes or native threads. When using a thread pool, multiple processes or native threads are created in advance. An example of this is the prefork mode in Apache. Otherwise, a process or native thread is spawn each time a connection is received. Fig XXX illustrates this.</p>
<div class="figure">
<img src="1.png" alt="Native threads" /><p class="caption">Native threads</p>
</div>
<p>The advantage of this architecture is that clear code can be written because the code is not decided into event handlers. Also, because the kernel assigns processes or native threads to available cores, we can balance utilization of cores. Its disadvantage is a large number of context switches between kernel and processes or native threads occur. So, performance gets poor.</p>
<h3 id="event-driven">Event driven</h3>
<p>Recently, it is said that event-driven programming is required to implement high performance servers. In this architecture multiple connections are handled by a single process (Fix XXX). Lighttpd is an example of web server using this architecture.</p>
<div class="figure">
<img src="2.png" alt="Event driven" /><p class="caption">Event driven</p>
</div>
<p>Since there is no need to switch processes, less context switches occur, and performance is improved. This is its chief advantage. However, it has two shortcomings. The first is the fact that only one core can be utilized because there is only a single process. The second is that it requires asynchronous programming, so code is fragmented into event handlers. Asynchronous programming also prevents the conventional use of exception handling (although there are no exceptions in C).</p>
<h3 id="process-per-core">1 process per core</h3>
<p>Many have hit upon the idea of creating N event-driven processes to utilize N cores (Fig XXX). Each process is called <em>worker</em>. A service port must be shared among workers. Using the prefork technique (please don't confuse with Apache's prefork mode), port sharing can be achieved by modifying code slightly.</p>
<div class="figure">
<img src="3.png" alt="1 process per core" /><p class="caption">1 process per core</p>
</div>
<p>One web server that uses this architecture is <code>nginx</code>. Node.js used the event-driven architecture in the past but it also implemented this scheme recently.</p>
<p>The advantage of this architecture is that it utilizes all cores and improves performance. However, it does not resolve the issue of programs having poor clarity.</p>
<h3 id="user-threads">User threads</h3>
<p>GHC's user threads can be used to solve the code clarity. They are implemented over an event-driven IO manager in GHC's runtime system. Starndard libraries of Haskell use non-blocking system calls so that they can cooperate with the IO manager. GHC's user threads are lightweight: modern computers can run 100,000 user threads smoothly. They are robust: even asynchronous exceptions are catch (we explain this later in detail).</p>
<p>Some languages and libraries provided user threads in the past, but they are not commonly used now because they are not lightweight or are not robust. But in Haskell, most computation is non-destructive. This means that almost all functions are thread-safe. GHC uses data allocation as a safe point of context switch of user threads. Because of functional programming style, new data are frequently created and it is known that <a href="http://www.aosabook.org/en/ghc.html">such data allocation is regulaly enough for context switching</a>.</p>
<p>Use of lightweight threads makes it possible to write code with good clarity like traditional thread programming while keeping high performance (Fig XXX).</p>
<div class="figure">
<img src="4.png" alt="User threads" /><p class="caption">User threads</p>
</div>
<p>As of this writing, <code>mighty</code> uses the prefork technique to fork processes to utilize cores and Warp does not have this functionality. Haskell community is now developing parallel IO manager. A Haskell program with parallel IO manager is executed as a single process and multiple IO managers run as native threads to utilize cores. And user threads are executed on one of cores. If it will be merged to GHC, Warp itself can use this architecture without any modifications.</p>
<h2 id="warps-architecture">Warp's architecture</h2>
<p>Warp is an HTTP engine for WAI (Web Application Interface). It runs WAI applications over HTTP. As we described before both Yesod and <code>mighty</code> are examples of WAI applications as illustrated in Fix XXX.</p>
<div class="figure">
<img src="wai.png" alt="WAI" /><p class="caption">WAI</p>
</div>
<p>The type of WAI applications is as follows:</p>
<pre><code>type Application = Request -&gt; ResourceT IO Response</code></pre>
<p>In Haskell, argument types of function are separated by right arrows and the most right one is the type of return value. So, we can interpret the definition as an application takes <code>Request</code> and returns <code>Response</code>.</p>
<p>After accepting a new HTTP connection, a dedicated user thread is spawn for the connection. It first receives an HTTP request from a client and parses it to <code>Request</code>. Then, Warp gives the <code>Request</code> to an application and takes a <code>Response</code> from it. Finally, Warp builds an HTTP response based on <code>Response</code> and sends it back to the client. This is illustrated in Fix XXX.</p>
<div class="figure">
<img src="warp.png" alt="Warp" /><p class="caption">Warp</p>
</div>
<p>The user thread repeats this procedure if necessary and terminates by itself when the connection is closed by the peer. It is also killed by the dedicated user thread for timeout if a significant amount of data is not received for a certain period.</p>
<h2 id="performance-of-warp">Performance of Warp</h2>
<p>Before we explain how to improve the performance of Warp, we would like to show the results of our benchmark. We measured throughput of <code>mighty</code> 2.8.2 (with Warp x.x.x) and <code>nginx</code> 1.2.4. Our benchmark environment is as follows:</p>
<ul>
<li>One &quot;12 cores&quot; machine (Intel Xeon E5645, two sockets, 6 cores per 1 CPU, two QPI between two CPUs)</li>
<li>Linux version 3.2.0 (Ubuntu 12.04 LTS), which is running directly on the machine (i.e. without a hypervisor)</li>
</ul>
<p>We tested several benchmark tools in the past and our favorite one was <code>httperf</code>. Since it uses <code>select()</code> and is just a single process program, it reaches its performance limits when we try to measure HTTP servers on multi-cores. So, we switched to <code>weighttp</code>, which is based on the <code>epoll</code> family and can use multiple native threads. We used <code>weighttp</code> as follows:</p>
<pre><code>weighttp -n 100000 -c 1000 -t 3 -k http://127.0.0.1:8000/</code></pre>
<p>This means that 1,000 HTTP connections are established and each connection sends 100 requests. 3 native threads are spawn to carry out these jobs..</p>
<p>For all requests, the same <code>index.html</code> file is returned. We used <code>nginx</code>'s <code>index.html</code> whose size is 151 bytes. As &quot;127.0.0.1&quot; suggests, We measured web servers locally. We should have measured from a remote machine but we don't have suitable environment at this moment. (NOTE: I'm planning to do benchmark using two machines soon.)</p>
<p>Since Linux has many control parameters, we need to configure the parameters carefully. You can find a good introduction about Linux parameter tuning in <a href="http://gwan.com/en_apachebench_httperf.html">ApacheBench &amp; HTTPerf</a>.</p>
<p>We carefully configured both <code>mighty</code> and <code>nginx</code> as follows:</p>
<ul>
<li>using file descriptor cache</li>
<li>no logging</li>
<li>no rate limitation</li>
</ul>
<p>Since our machine has 12 cores and <code>weighttp</code> uses three native threads, we measured web servers from one worker to eight workers(to our experience, three native thread is enough to measure 8 workers). Here is the result:</p>
<div class="figure">
<img src="multi-workers.png" alt="Performance of Warp and nginx" /><p class="caption">Performance of Warp and <code>nginx</code></p>
</div>
<p>X-axis is the number of workers and y-axis means throughput whose unit is requests per second.</p>
<h2 id="key-ideas">Key ideas</h2>
<p>There are three key ideas to implement high-performance server in Haskell:</p>
<ol style="list-style-type: decimal">
<li>Issuing as few system calls as possible</li>
<li>Specialization and avoiding re-calculation</li>
<li>Avoiding locks</li>
</ol>
<h3 id="issuing-as-few-system-calls-as-possible">Issuing as few system calls as possible</h3>
<p>If a system call is issued, CPU time is given to kernel and all user threads stop. So, we need to use as few system calls as possible. For a HTTP session to get a static file, Warp calls <code>recv()</code>, <code>send()</code> and <code>sendfile()</code> only (Fig warp.png). <code>open()</code>, <code>stat()</code>, <code>close()</code> and other system calls can be committed thanks to cache mechanism described in Section XXX.</p>
<p>We can use <code>strace</code> to see what system calls are actually used. When we observed behavior of <code>nginx</code> with <code>strace</code>, we noticed that it uses <code>accept4()</code>, about which we don't know at that time.</p>
<p>Using Haskell's standard network library, a listening socket is created with the non-blocking flag set. When a new connection is accepted from the listening socket, it is necessary to set the corresponding socket as non-blocking, too. The <code>network</code> package implements this by calling <code>fcntl()</code> twice: one is to get the current flags and the other is to set the flags with the non-blocking flag <em>ORed</em>.</p>
<p>On Linux, the non-block flag of a connected socket is always unset even if its listening socket is non-blocking. <code>accept4()</code> is an extension version of <code>accept()</code> on Linux. It can set the non-blocking flag when accepting. So, if we use <code>accept4()</code>, we can avoid two unnecessary <code>fcntl()</code>s. Our patch to use <code>accept4()</code> on Linux has been already merged to the network library.</p>
<h3 id="specialization-and-avoiding-re-calculation">Specialization and avoiding re-calculation</h3>
<p>GHC provides profiling mechanism but it has a limitation: right profiling is possible if a program runs in foreground and it does not spawn child processes. So, if we want to profile live activities of servers, we need to implement special care for profiling.</p>
<p><code>mighty</code> has this mechanism. Suppose that N is the number of workers in the configuration file of <code>mighty</code>. If N is larger than or equal to 2, <code>mighty</code> creates N child processes and the parent process just works to deliver signals. However, if N is 1, <code>mighty</code> does not creates one child process. The executed process itself serves HTTP. Also, <code>mighty</code> stays its terminal if debug mode is on.</p>
<p>When we took profile of <code>mighty</code>, we surprised that the standard function to format date string consumes most CPU time. As many know, an end HTTP server should return GMT date strings in header fields such as Date:, Last-Modified:, etc:</p>
<pre><code>Date: Mon, 01 Oct 2012 07:38:50 GMT</code></pre>
<p>So, we implemented a special formatter to generate GMT date strings. Comparing the standard function and our specialized function with <code>criterion</code>, a standard benchmark library of Haskell, ours are much faster. But if an HTTP server accepts more than one request per second, the server repeats the same formatting again and again. So, we also implemented cache mechanism for date strings.</p>
<p>TBD: reference to other parts.</p>
<h3 id="avoiding-locks">Avoiding locks</h3>
<p>Unnecessary locks are evil for programming. Our code sometime uses unnecessary locks imperceptibly because runtime systems or libraries uses locks deep inside. To implement high-performance server, we need to identify locks and avoid locks if possible. It is worth pointing out that locks will become much more critical under the parallel IO manager. We will talk how to identify and avoid locks in Section XXX and Section XXX.</p>
<h2 id="http-request-parser">HTTP request parser</h2>
<ul>
<li>Parser generator vs handmade parser</li>
<li>No timeout care thanks to timeout manager -- From &quot;Warp: A Haskell Web Server&quot;?</li>
<li>Conduit</li>
</ul>
<h2 id="http-response-composer">HTTP response composer</h2>
<p><code>Response</code> of WAI has three constructors:</p>
<pre><code>ResponseBuilder Status ResponseHeaders Builder
ResponseSource Status ResponseHeaders (Source (ResourceT IO) (Flush Builder))
ResponseFile Status ResponseHeaders FilePath (Maybe FilePart)</code></pre>
<p>For <code>ResponseBuilder</code> and <code>ResponseSource</code>, send() is used to send both an HTTP response header and body with a fixed buffer. For <code>ResponseFile</code>, Warp uses send() and sendfile() to send an HTTP response header and body, respectively. Fig XXX illustrates this case.</p>
<h3 id="composer-for-http-response-header">Composer for HTTP response header</h3>
<p>The old composer for HTTP response header creates a <code>Builder</code> of the blaze-builder package by appending the <code>Bytestring</code>s in the <code>Status</code> and the <code>ResponseHeaders</code>. Each append operation runs in O(1). The <code>Builder</code> is converted to a list of <em>packed</em> <code>ByteString</code>s and sent with <code>writev()</code>. And then a file (HTTP response body) is sent with <code>sendfile()</code>.</p>
<p>In many cases, the performance of the blaze builder is sufficient. But I suspected that it is not fast enough for high performance servers.</p>
<h3 id="composer-for-http-response-body">Composer for HTTP response body</h3>
<p>TBD</p>
<h3 id="sending-header-and-body-together">Sending header and body together</h3>
<p>When we measured the performance of Warp, we always did it with high concurrency. That is, we always make multiple connections at the same time. It gave us a good result. However, when we set the number of concurrency to 1, we found Warp is really really slow.</p>
<p>Observing the results of <code>tcpdump</code>, we realized that this is because old Warp uses the combination of writev() for header and sendfile() for body. In this case, an HTTP header and body are sent in separate TCP packets (Fig xxx).</p>
<div class="figure">
<img src="tcpdump.png" alt="Packet sequence of old Warp" /><p class="caption">Packet sequence of old Warp</p>
</div>
<p>To send them in a single TCP packet (when possible), new Warp switched from <code>writev()</code> to <code>send()</code>. It uses <code>send()</code> with the <code>MSG_MORE</code> flag to store a header and <code>sendfile()</code> to send both the stored header and a file. This made the throughput at least 100 times faster.</p>
<h2 id="clean-up-with-timers">Clean-up with timers</h2>
<p>This section explain how to implement connection timeout and how to cache file descriptors.</p>
<h3 id="timers-for-connections">Timers for connections</h3>
<p>To prevent slowloris attacks, communication with a client should be canceled if the client does not send a significant amount of data for a certain period. Haskell provides a standard function called <code>timeout</code> whose type is as follows:</p>
<pre><code>Int -&gt; IO a -&gt; IO (Maybe a)</code></pre>
<p>The first argument is time to timeout in microsecond. The second argument is an action which handles input/output (IO). This function returns a value of <code>Maybe a</code> in the IO context. <code>Maybe</code> is defined as follows:</p>
<pre><code>data Maybe a = Nothing | Just a</code></pre>
<p><code>Nothing</code> means an error (without reason information) and <code>Just</code> encloses a successful value <code>a</code>. So, <code>timeout</code> returns <code>Nothing</code> if an action is completed in a specified time. Otherwise, a successful value is returned wrapped with <code>Just</code>. <code>timeout</code> eloquently shows how high Haskell's composability is.</p>
<p>Unfortunately, <code>timeout</code> spawns a user thread to handle timeout. To implement high-performance servers, we need to avoid to create a user thread for timeout for each connection.</p>
<p>So, we implement a timeout system which uses only one user thread to handle timeout of all connections. Its heart is the following two points:</p>
<ul>
<li>Double <code>IORef</code>s</li>
<li>Safe swap and merge algorithm</li>
</ul>
<p>Suppose that status of connections is described as <code>Active</code> and <code>Inactive</code>. To clean up inactive connections, a dedicated Haskell thread, called the timeout manager, repeatedly inspects the status of each connection. If status is <code>Active</code>, the timeout manager turns it to <code>Inactive</code>. If <code>Inactive</code>, the timeout manager kills its associated Haskell thread.</p>
<p>Each status is refereed by an <code>IORef</code>. To update status through this <code>IORef</code>, atomicity is not necessary because status is just overwritten. In addition to the timeout manager, each Haskell thread repeatedly turns its status to <code>Active</code> through its own <code>IORef</code> if its connection actively continues.</p>
<p>To check all statuses easily, the timeout manager uses a list of the <code>IORef</code> to status. A Haskell thread spawned for a new connection tries to 'cons' its new <code>IORef</code> for an <code>Active</code> status to the list. So, the list is a critical section and we need atomicity to keep the list consistent.</p>
<div class="figure">
<img src="timeout.png" alt="A list of status. A and I indicates Active and Inactive, respectively" /><p class="caption">A list of status. <code>A</code> and <code>I</code> indicates <code>Active</code> and <code>Inactive</code>, respectively</p>
</div>
<p>A standard way to keep consistency in Haskell is <code>MVar</code>. But <code>MVar</code> is slow because each <code>MVar</code> is protected with a home-brewed spin lock. So, he used another <code>IORef</code> to refer the list and <code>atomicModifyIORef</code> to manipulate it. <code>IORef</code> is a reference whose value can be destructively updated. <code>atomicModifyIORef</code> is a function to atomically update <code>IORef</code>'s values. It is fast since it is implemented via CAS (Compare-and-Swap), which is much faster than spin locks.</p>
<p>The following is the outline of the safe swap and merge algorithm:</p>
<pre><code>do xs &lt;- atomicModifyIORef ref (\ys -&gt; ([], ys)) -- swap with an empty list, []
   xs&#39; &lt;- manipulates_status xs
   atomicModifyIORef ref (\ys -&gt; (merge xs&#39; ys, ()))</code></pre>
<p>The timeout manager atomically swaps the list with an empty list. Then it manipulates the list by turning status and/or removing unnecessary status for killed Haskell threads. During this process, new connections may be created and their status are inserted with <code>atomicModifyIORef</code> by corresponding Haskell threads. Then, the timeout manager atomically merges the pruned list and the new list.</p>
<h3 id="timers-for-file-descriptors">Timers for file descriptors</h3>
<p>Let's consider the case where Warp sends the entire file by <code>sendfile()</code>. Unfortunately, we need to call <code>stat()</code> to know the size of the file because <code>sendfile()</code> on Linux requires the caller to specify how many bytes to be sent (<code>sendfile()</code> on FreeBSD/MacOS has magic number '0' which indicates the end of file).</p>
<p>If WAI applications know the file size, Warp can avoid <code>stat()</code>. It is easy for WAI applications to cache file information such as size and modification time. If cache timeout is fast enough (say 10 seconds), the risk of cache inconsistency problem is not serious. And because we can safely clean up the cache, we don't have to worry about leakage.</p>
<p>Since <code>sendfile()</code> requires a file descriptor, the naive sequence to send a file is <code>open()</code>, <code>sendfile()</code> repeatedly if necessary, and <code>close()</code>. In this section, we consider how to cache file descriptors to avoid <code>open()</code> and <code>close()</code>. Caching file descriptors should work as follows: If a client requests that a file be sent, a file descriptor is opened by <code>open()</code>. And if another client requests the same file shortly thereafter, the file descriptor is reused. At a later time, the file descriptor is closed by <code>close()</code> if no user thread uses it.</p>
<p>A typical tactic for this case is reference counter. But we was not sure that we could implement a robust mechanism for a reference counter. What happens if a user thread is killed for unexpected reasons? If we fail to decrement its reference counter, the file descriptor leaks. We noticed that the scheme of connection timeout is safe to reuse as a cache mechanism for file descriptors because it does not use reference counters. However, we cannot simply reuse Warp's timeout code for some reasons:</p>
<p>Each user thread has its own status. So, status is not shared. But we would like to cache file descriptors to avoid <code>open()</code> and <code>close()</code> by sharing. So, we need to search a file descriptor for a requested file from cached ones. Since this look-up should be fast, we should not use a list. Also, because requests are received concurrently, two or more file descriptors for the same file may be opened. So, we need to store multiple file descriptors for a single file name. This is technically called a <em>multimap</em>.</p>
<p>We implemented a multimap whose look-up is O(log N) and pruning is O(N) with red-black trees whose node contains a non-empty list. Since a red-black trees is one of binary search trees, look-up is O(log N) where N is the number of nodes. Also, we can translate it into an order list in O(log N). In our implementation, pruning nodes which contains a file descriptor to be closed is also done during this procedure. An algorithm is known, which converts a order list to a red-black tree in O(N).</p>
<h2 id="future-work">Future work</h2>
<p>We have some items to improve Warp in the future but we will explain two here.</p>
<h3 id="memory-allocation">Memory allocation</h3>
<p>When receiving and sending packets, buffers are allocated. We think that these memory allocations may be the current bottleneck. GHC runtime system uses <code>pthread_mutex_lock</code> to obtain a large object (larger than 409 bytes in 64 bit machines).</p>
<p>We tried to measure how much memory allocation for HTTP response header consume time. We copied the <code>create</code> function of <code>ByteString</code> to Warp and surrounded <code>mallocByteString</code> with <code>Debug.Trace.traceEventIO</code>. Then we complied <code>mighty</code> with it and took eventlog. The result eventlog is illustrated as follows:</p>
<div class="figure">
<img src="eventlog.png" alt="eventlog" /><p class="caption">eventlog</p>
</div>
<p>Brick red bars indicates the event created by <code>traceEventIO</code>. The area surrounded by two bars is the time consumed by <code>mallocByteString</code>. It is about 1/10 of an HTTP session. We are confident that the same thing happens when allocating receiving buffers.</p>
<h3 id="new-thundering-herd">New thundering herd</h3>
<p>Thundering herd is an old but new problem. Suppose that processes/native threads are pre-forked to share a listening socket. They call <code>accept()</code> on the socket. When a connection is created, old Linux and FreeBSD wakes up all of them. And only one can accept it and the others sleeps again. Since this causes many context switches, we face performance problem. This is called <em>thundering</em> <em>herd</em>. Recent Linux and FreeBSD wakes up only one process/native thread. So, this problem became a thing of the past.</p>
<p>Recent network servers tend to use the <code>epoll</code> family. If worker processes share a listen socket and they manipulate accept connections through the <code>epoll</code> family, thundering herd appears again. This is because the semantics of the <code>epoll</code> family is to notify all processes/native threads. <code>nginx</code> and <code>mighty</code> are victims of new thundering herd.</p>
<p>The parallel IO manager is free from new thundering herd. In this architecture, only one IO manager accepts new connections through the <code>epoll</code> family. And other IO managers handle established connections.</p>
<h2 id="conclusion">Conclusion</h2>
